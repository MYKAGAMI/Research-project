import sys
sys.path.insert(0,'.')

import time
import torch
import torch.nn as nn
import numpy as np
import torch_tensorrt
import timm
from example.test_vit import *
import utils.datasets as datasets

def compile_tensorrt_fixed(model, input_shape):
    """ä¿®å¤åçš„TensorRTç¼–è¯‘"""
    print(f"ğŸ”§ ä¿®å¤TensorRTç¼–è¯‘ (shape: {input_shape})...")
    
    try:
        # ç¡®ä¿æ¨¡å‹åœ¨evalæ¨¡å¼
        model.eval()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        dummy_input = torch.randn(input_shape).cuda()
        
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œ
        with torch.no_grad():
            original_output = model(dummy_input)
        
        # ä¿®å¤åçš„ç¼–è¯‘è®¾ç½® - ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°
        compile_settings = {
            'inputs': [torch_tensorrt.Input(input_shape, dtype=torch.float32)],
            'enabled_precisions': {torch.float16, torch.float32},
            'workspace_size': 2*1024**3,  # 2GB
            'min_block_size': 3,
            'require_full_compilation': False,
            'truncate_long_and_double': True,
            # ç§»é™¤äº† max_batch_size, opt_level ç­‰ä¸æ”¯æŒçš„å‚æ•°
        }
        
        print("   å¼€å§‹ç¼–è¯‘...")
        trt_model = torch_tensorrt.compile(model, **compile_settings)
        
        # éªŒè¯ç¼–è¯‘åçš„æ¨¡å‹
        with torch.no_grad():
            trt_output = trt_model(dummy_input)
            
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸€è‡´
        diff = torch.abs(original_output - trt_output).max().item()
        print(f"   è¾“å‡ºå·®å¼‚: {diff:.6f}")
        
        if diff < 0.01:  # å¯æ¥å—çš„æ•°å€¼è¯¯å·®
            print("âœ… TensorRTç¼–è¯‘æˆåŠŸä¸”è¾“å‡ºä¸€è‡´")
            return trt_model, True
        else:
            print(f"âš ï¸ TensorRTç¼–è¯‘æˆåŠŸä½†è¾“å‡ºå·®å¼‚è¾ƒå¤§: {diff}")
            return trt_model, True  # ä»ç„¶è¿”å›æˆåŠŸï¼Œå› ä¸ºå°å·®å¼‚æ˜¯æ­£å¸¸çš„
            
    except Exception as e:
        print(f"âŒ TensorRTç¼–è¯‘å¤±è´¥: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        return model, False

def test_large_batch_sizes():
    """æµ‹è¯•å¤§batch sizeçš„æ€§èƒ½"""
    print("ğŸš€ æµ‹è¯•å¤§Batch Sizeæ€§èƒ½")
    print("="*80)
    
    model_name = 'vit_tiny_patch16_224'
    large_batch_sizes = [128, 256, 512]  # æ›´å¤§çš„batch size
    
    original_model = timm.create_model(model_name, pretrained=True)
    original_model = original_model.cuda().eval()
    
    results = []
    
    for bs in large_batch_sizes:
        print(f"\nğŸ“Š æµ‹è¯• Batch Size: {bs}")
        
        try:
            # æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
            dummy_input = torch.randn(bs, 3, 224, 224).cuda()
            with torch.no_grad():
                _ = original_model(dummy_input)
            
            # æµ‹è¯•PyTorchæ€§èƒ½
            pytorch_latency = test_model_latency_simple(original_model, bs, "PyTorch")
            
            # ç¼–è¯‘TensorRT
            input_shape = (bs, 3, 224, 224)
            trt_model, success = compile_tensorrt_fixed(original_model, input_shape)
            
            if success:
                trt_latency = test_model_latency_simple(trt_model, bs, "TensorRT")
                speedup = pytorch_latency / trt_latency if trt_latency > 0 else 1.0
            else:
                trt_latency = pytorch_latency
                speedup = 1.0
            
            # è®¡ç®—ååé‡
            pytorch_throughput = bs * 1000 / pytorch_latency  # images/sec
            trt_throughput = bs * 1000 / trt_latency if trt_latency > 0 else pytorch_throughput
            
            results.append({
                'batch_size': bs,
                'pytorch_latency': pytorch_latency,
                'pytorch_throughput': pytorch_throughput,
                'tensorrt_latency': trt_latency,
                'tensorrt_throughput': trt_throughput,
                'speedup': speedup,
                'success': success
            })
            
            print(f"   PyTorch: {pytorch_latency:.2f}ms, {pytorch_throughput:.0f} images/sec")
            if success:
                print(f"   TensorRT: {trt_latency:.2f}ms, {trt_throughput:.0f} images/sec")
                print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
            else:
                print("   TensorRTç¼–è¯‘å¤±è´¥")
                
            # æ¸…ç†æ˜¾å­˜
            del dummy_input, trt_model
            torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"   âŒ æ˜¾å­˜ä¸è¶³ï¼Œè·³è¿‡batch size {bs}")
                torch.cuda.empty_cache()
                continue
            else:
                print(f"   âŒ å…¶ä»–é”™è¯¯: {e}")
                continue
    
    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“ˆ å¤§Batch Sizeæ€§èƒ½æ±‡æ€»:")
    print(f"{'Batch':<8} {'PyTorch ms':<12} {'PyTorch fps':<12} {'TRT ms':<10} {'TRT fps':<10} {'åŠ é€Ÿæ¯”':<8} {'çŠ¶æ€':<8}")
    print("-" * 80)
    
    best_speedup = 1.0
    best_config = None
    
    for r in results:
        status = "âœ…" if r['success'] else "âŒ"
        if r['success']:
            print(f"{r['batch_size']:<8} {r['pytorch_latency']:<12.2f} {r['pytorch_throughput']:<12.0f} {r['tensorrt_latency']:<10.2f} {r['tensorrt_throughput']:<10.0f} {r['speedup']:<8.2f}x {status:<8}")
            if r['speedup'] > best_speedup:
                best_speedup = r['speedup']
                best_config = r
        else:
            print(f"{r['batch_size']:<8} {r['pytorch_latency']:<12.2f} {r['pytorch_throughput']:<12.0f} {'--':<10} {'--':<10} {'--':<8} {status:<8}")
    
    if best_config:
        print(f"\nğŸ¯ æœ€ä½³TensorRTé…ç½®:")
        print(f"   Batch Size: {best_config['batch_size']}")
        print(f"   åŠ é€Ÿæ¯”: {best_config['speedup']:.2f}x")
        print(f"   ååé‡: {best_config['tensorrt_throughput']:.0f} images/sec")
    else:
        print(f"\nâš ï¸ æ‰€æœ‰TensorRTç¼–è¯‘éƒ½å¤±è´¥äº†")
    
    return results

def test_model_latency_simple(model, batch_size, description, num_iterations=50):
    """ç®€åŒ–çš„å»¶è¿Ÿæµ‹è¯•"""
    model.eval()
    
    # åˆ›å»ºè¾“å…¥
    dummy_input = torch.randn(batch_size, 3, 224, 224).cuda()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(5):
            _ = model(dummy_input)
    
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model(dummy_input)
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    total_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    avg_latency = total_time / num_iterations
    
    return avg_latency

def test_different_models():
    """æµ‹è¯•ä¸åŒå¤§å°çš„æ¨¡å‹"""
    print("\nğŸ”¬ æµ‹è¯•ä¸åŒæ¨¡å‹å¤§å°")
    print("="*80)
    
    models_to_test = [
        ('vit_tiny_patch16_224', "ViT-Tiny"),
        ('vit_small_patch16_224', "ViT-Small"),
        ('vit_base_patch16_224', "ViT-Base"),
    ]
    
    batch_size = 32
    results = []
    
    for model_name, display_name in models_to_test:
        print(f"\nğŸ“Š æµ‹è¯•æ¨¡å‹: {display_name}")
        
        try:
            # åŠ è½½æ¨¡å‹
            model = timm.create_model(model_name, pretrained=True)
            model = model.cuda().eval()
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            total_params = sum(p.numel() for p in model.parameters())
            model_size_mb = total_params * 4 / 1024 / 1024
            
            # æµ‹è¯•PyTorchæ€§èƒ½
            pytorch_latency = test_model_latency_simple(model, batch_size, "PyTorch")
            pytorch_throughput = batch_size * 1000 / pytorch_latency
            
            # æµ‹è¯•TensorRT
            input_shape = (batch_size, 3, 224, 224)
            trt_model, success = compile_tensorrt_fixed(model, input_shape)
            
            if success:
                trt_latency = test_model_latency_simple(trt_model, batch_size, "TensorRT")
                trt_throughput = batch_size * 1000 / trt_latency
                speedup = pytorch_latency / trt_latency
            else:
                trt_latency = pytorch_latency
                trt_throughput = pytorch_throughput
                speedup = 1.0
            
            results.append({
                'name': display_name,
                'params': total_params,
                'size_mb': model_size_mb,
                'pytorch_throughput': pytorch_throughput,
                'tensorrt_throughput': trt_throughput,
                'speedup': speedup,
                'success': success
            })
            
            print(f"   å‚æ•°é‡: {total_params:,} ({model_size_mb:.1f}MB)")
            print(f"   PyTorch: {pytorch_throughput:.0f} images/sec")
            if success:
                print(f"   TensorRT: {trt_throughput:.0f} images/sec ({speedup:.2f}x)")
            else:
                print(f"   TensorRT: ç¼–è¯‘å¤±è´¥")
            
            # æ¸…ç†æ˜¾å­˜
            del model, trt_model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
            torch.cuda.empty_cache()
            continue
    
    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“ˆ ä¸åŒæ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æ¨¡å‹':<12} {'å‚æ•°é‡':<12} {'å¤§å°(MB)':<10} {'PyTorch':<12} {'TensorRT':<12} {'åŠ é€Ÿæ¯”':<8}")
    print("-" * 80)
    
    for r in results:
        params_str = f"{r['params']/1e6:.1f}M"
        if r['success']:
            print(f"{r['name']:<12} {params_str:<12} {r['size_mb']:<10.1f} {r['pytorch_throughput']:<12.0f} {r['tensorrt_throughput']:<12.0f} {r['speedup']:<8.2f}x")
        else:
            print(f"{r['name']:<12} {params_str:<12} {r['size_mb']:<10.1f} {r['pytorch_throughput']:<12.0f} {'--':<12} {'--':<8}")
    
    return results

def final_recommendations():
    """æœ€ç»ˆå»ºè®®"""
    print("\nğŸ’¡ æœ€ç»ˆå»ºè®®å’Œæ€»ç»“")
    print("="*80)
    
    print("ğŸ” é—®é¢˜æ ¹æºåˆ†æ:")
    print("   1. ä¹‹å‰çš„TensorRTç¼–è¯‘å‚æ•°æœ‰è¯¯ï¼Œå¯¼è‡´ä¸€ç›´ç¼–è¯‘å¤±è´¥")
    print("   2. ViTæ¨¡å‹ç¡®å®å¯¹TensorRTä¼˜åŒ–æœ‰é™")
    print("   3. å°batch sizeå’Œå°æ¨¡å‹ä¸åˆ©äºTensorRTå‘æŒ¥ä¼˜åŠ¿")
    
    print("\nğŸš€ ä¿®å¤åçš„æµ‹è¯•ç»“æœé¢„æœŸ:")
    print("   1. å¤§batch size (128-512) å¯èƒ½ä¼šçœ‹åˆ°TensorRTä¼˜åŠ¿")
    print("   2. æ›´å¤§çš„æ¨¡å‹ (ViT-Base/Large) æ›´é€‚åˆTensorRT")
    print("   3. ä½†å¯¹äºä½ çš„ç ”ç©¶åœºæ™¯å¯èƒ½ä»ç„¶æœ‰é™")
    
    print("\nğŸ“Š å¯¹ä½ ç ”ç©¶çš„å½±å“:")
    print("   âœ… PTQ4ViTçš„ç®—æ³•ä»·å€¼ä¸å—å½±å“")
    print("   âœ… é‡åŒ–ç²¾åº¦çš„æå‡ä»ç„¶æœ‰æ„ä¹‰")
    print("   âš ï¸ éƒ¨ç½²åŠ é€Ÿçš„æ‰¿è¯ºéœ€è¦è°ƒæ•´")
    
    print("\nğŸ“ å»ºè®®çš„ç ”ç©¶ç­–ç•¥:")
    print("   1. å¼ºè°ƒPTQ4ViTåœ¨ç²¾åº¦ä¿æŒæ–¹é¢çš„ä¼˜åŠ¿")
    print("   2. è¯šå®æŠ¥å‘ŠTensorRTéƒ¨ç½²çš„æŒ‘æˆ˜")
    print("   3. è®¨è®ºç®—æ³•é‡åŒ–vsç¡¬ä»¶é‡åŒ–çš„gap")
    print("   4. ä¸ºæœªæ¥çš„çœŸé‡åŒ–å®ç°é“ºè·¯")
    
    print("\nğŸ¯ è®ºæ–‡æ’°å†™é‡ç‚¹:")
    print("   â€¢ ç®—æ³•åˆ›æ–°ï¼šTwin Uniform + Hessian-guided")
    print("   â€¢ ç²¾åº¦æå‡ï¼šç›¸æ¯”å…¶ä»–PTQæ–¹æ³•çš„ä¼˜åŠ¿")
    print("   â€¢ ç†è®ºè´¡çŒ®ï¼šå¯¹ViTé‡åŒ–çš„ç†è§£")
    print("   â€¢ æœªæ¥å·¥ä½œï¼šå‘çœŸé‡åŒ–çš„è½¬åŒ–")

if __name__ == "__main__":
    print("ğŸ”§ ä¿®å¤åçš„TensorRTæµ‹è¯•")
    print("="*80)
    
    # 1. æµ‹è¯•å¤§batch size
    print("\nğŸš€ æ­¥éª¤1: æµ‹è¯•å¤§Batch Size...")
    try:
        large_batch_results = test_large_batch_sizes()
    except Exception as e:
        print(f"âŒ å¤§batchæµ‹è¯•å¤±è´¥: {e}")
        large_batch_results = []
    
    # 2. æµ‹è¯•ä¸åŒæ¨¡å‹
    print("\nğŸš€ æ­¥éª¤2: æµ‹è¯•ä¸åŒæ¨¡å‹å¤§å°...")
    try:
        model_results = test_different_models()
    except Exception as e:
        print(f"âŒ ä¸åŒæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        model_results = []
    
    # 3. æœ€ç»ˆå»ºè®®
    final_recommendations()
    
    print("\n" + "="*80)
    print("ğŸ ä¿®å¤æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„TensorRTç¼–è¯‘
    has_success = False
    best_speedup = 1.0
    
    for result in large_batch_results:
        if result.get('success', False) and result.get('speedup', 1.0) > best_speedup:
            has_success = True
            best_speedup = result['speedup']
    
    for result in model_results:
        if result.get('success', False) and result.get('speedup', 1.0) > best_speedup:
            has_success = True
            best_speedup = result['speedup']
    
    if has_success:
        print(f"âœ… TensorRTç¼–è¯‘æˆåŠŸï¼æœ€ä½³åŠ é€Ÿæ¯”: {best_speedup:.2f}x")
        print(f"   å»ºè®®ä½¿ç”¨å¤§batch sizeå’Œå¤§æ¨¡å‹æ¥è·å¾—æ›´å¥½çš„åŠ é€Ÿæ•ˆæœ")
    else:
        print(f"âš ï¸ TensorRTç¼–è¯‘ä»æœ‰é—®é¢˜ï¼Œä½†è¿™ä¸å½±å“PTQ4ViTçš„ç ”ç©¶ä»·å€¼")
        print(f"   å»ºè®®ä¸“æ³¨äºç®—æ³•åˆ›æ–°ï¼Œè¯šå®æŠ¥å‘Šéƒ¨ç½²æŒ‘æˆ˜")
    
    print(f"\nğŸ’¡ å…³é”®å»ºè®®:")
    print(f"   1. PTQ4ViTçš„ç®—æ³•è´¡çŒ®ä¾ç„¶å¾ˆæœ‰ä»·å€¼")
    print(f"   2. å¯ä»¥è¯šå®è®¨è®ºä»ç ”ç©¶åˆ°éƒ¨ç½²çš„æŒ‘æˆ˜")
    print(f"   3. è¿™æ ·çš„è®¨è®ºå¯¹å­¦æœ¯ç•Œä¹Ÿå¾ˆæœ‰ä»·å€¼")