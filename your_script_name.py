import sys
sys.path.insert(0,'.')

import time
import torch
import torch.nn as nn
import numpy as np
import torch_tensorrt
import timm
import utils.datasets as datasets
import utils.net_wrap as net_wrap
from utils.quant_calib import HessianQuantCalibrator
from utils.models import get_net
from configs.PTQ4ViT import get_module

def apply_ptq4vit_quantization(model_name, calib_size=32):
    """åº”ç”¨PTQ4ViTé‡åŒ–"""
    print(f"ğŸ”§ åº”ç”¨PTQ4ViTé‡åŒ–åˆ° {model_name}")
    
    # 1. åŠ è½½åŸå§‹æ¨¡å‹
    net = get_net(model_name)
    
    # 2. åŒ…è£…é‡åŒ–å±‚
    # åˆ›å»ºé‡åŒ–é…ç½®
    class QuantConfig:
        def __init__(self):
            self.bit = 8
            self.conv_fc_name_list = ["qconv", "qlinear_qkv", "qlinear_proj", "qlinear_MLP_1", "qlinear_MLP_2", "qlinear_classifier", "qlinear_reduction"]
            self.matmul_name_list = ["qmatmul_qk", "qmatmul_scorev"]
            self.w_bit = {name: 8 for name in self.conv_fc_name_list}
            self.a_bit = {name: 8 for name in self.conv_fc_name_list}
            self.A_bit = {name: 8 for name in self.matmul_name_list}
            self.B_bit = {name: 8 for name in self.matmul_name_list}
            
        def get_module(self, module_type, *args, **kwargs):
            return get_module(module_type, *args, **kwargs)
    
    quant_cfg = QuantConfig()
    wrapped_modules = net_wrap.wrap_modules_in_net(net, quant_cfg)
    
    # 3. å‡†å¤‡æ ¡å‡†æ•°æ®
    g = datasets.ViTImageNetLoaderGenerator('/mnt/ino-raid4/usrs/feng/ImageNet', 'imagenet', 32, 32, 16, kwargs={"model": net})
    calib_loader = g.calib_loader(num=calib_size)
    
    # 4. è¿è¡Œé‡åŒ–æ ¡å‡†
    print("   å¼€å§‹é‡åŒ–æ ¡å‡†...")
    quant_calibrator = HessianQuantCalibrator(net, wrapped_modules, calib_loader, sequential=False, batch_size=4)
    quant_calibrator.batching_quant_calib()
    
    print("âœ… PTQ4ViTé‡åŒ–å®Œæˆ")
    return net

def compile_tensorrt_int8(model, input_shape, use_int8=True):
    """TensorRTç¼–è¯‘ï¼ˆæ”¯æŒint8ï¼‰"""
    print(f"ğŸ”§ TensorRTç¼–è¯‘ {'INT8' if use_int8 else 'FP16'} (shape: {input_shape})...")
    
    try:
        model.eval()
        dummy_input = torch.randn(input_shape).cuda()
        
        # æµ‹è¯•åŸå§‹æ¨¡å‹
        with torch.no_grad():
            original_output = model(dummy_input)
        
        # ç¼–è¯‘è®¾ç½®
        if use_int8:
            compile_settings = {
                'inputs': [torch_tensorrt.Input(input_shape, dtype=torch.float32)],
                'enabled_precisions': {torch.int8, torch.float16, torch.float32},  # å¯ç”¨int8
                'workspace_size': 2*1024**3,
                'min_block_size': 3,
                'require_full_compilation': False,
                'truncate_long_and_double': True,
            }
        else:
            compile_settings = {
                'inputs': [torch_tensorrt.Input(input_shape, dtype=torch.float32)],
                'enabled_precisions': {torch.float16, torch.float32},  # ä»…FP16+FP32
                'workspace_size': 2*1024**3,
                'min_block_size': 3,
                'require_full_compilation': False,
                'truncate_long_and_double': True,
            }
        
        print("   å¼€å§‹ç¼–è¯‘...")
        trt_model = torch_tensorrt.compile(model, **compile_settings)
        
        # éªŒè¯ç¼–è¯‘åçš„æ¨¡å‹
        with torch.no_grad():
            trt_output = trt_model(dummy_input)
            
        # æ£€æŸ¥è¾“å‡ºå·®å¼‚
        diff = torch.abs(original_output - trt_output).max().item()
        print(f"   è¾“å‡ºå·®å¼‚: {diff:.6f}")
        
        if diff < 0.1:  # å¯æ¥å—çš„æ•°å€¼è¯¯å·®
            print(f"âœ… TensorRT {'INT8' if use_int8 else 'FP16'} ç¼–è¯‘æˆåŠŸ")
            return trt_model, True
        else:
            print(f"âš ï¸ TensorRTç¼–è¯‘æˆåŠŸä½†è¾“å‡ºå·®å¼‚è¾ƒå¤§: {diff}")
            return trt_model, True
            
    except Exception as e:
        print(f"âŒ TensorRTç¼–è¯‘å¤±è´¥: {e}")
        return model, False

def test_model_latency(model, batch_size, description, num_iterations=50):
    """æµ‹è¯•æ¨¡å‹å»¶è¿Ÿ"""
    model.eval()
    dummy_input = torch.randn(batch_size, 3, 224, 224).cuda()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(5):
            _ = model(dummy_input)
    
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    start_time = time.time()
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model(dummy_input)
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    total_time = (end_time - start_time) * 1000
    avg_latency = total_time / num_iterations
    
    return avg_latency

def compare_quantization_acceleration():
    """å¯¹æ¯”é‡åŒ–å‰åçš„TensorRTåŠ é€Ÿæ•ˆæœ"""
    print("ğŸš€ å¯¹æ¯”é‡åŒ–å‰åçš„TensorRTåŠ é€Ÿæ•ˆæœ")
    print("="*80)
    
    model_name = 'vit_small_patch16_224'
    batch_sizes = [32, 128]
    
    results = []
    
    for bs in batch_sizes:
        print(f"\nğŸ“Š æµ‹è¯• Batch Size: {bs}")
        
        try:
            # 1. åŸå§‹FP32æ¨¡å‹
            print("1ï¸âƒ£ æµ‹è¯•åŸå§‹FP32æ¨¡å‹")
            original_model = timm.create_model(model_name, pretrained=True).cuda().eval()
            fp32_latency = test_model_latency(original_model, bs, "FP32")
            fp32_throughput = bs * 1000 / fp32_latency
            print(f"   FP32: {fp32_latency:.2f}ms, {fp32_throughput:.0f} images/sec")
            
            # 2. FP16 TensorRTä¼˜åŒ–
            print("2ï¸âƒ£ æµ‹è¯•FP16 TensorRTä¼˜åŒ–")
            input_shape = (bs, 3, 224, 224)
            trt_fp16_model, success = compile_tensorrt_int8(original_model, input_shape, use_int8=False)
            if success:
                fp16_latency = test_model_latency(trt_fp16_model, bs, "TensorRT-FP16")
                fp16_throughput = bs * 1000 / fp16_latency
                fp16_speedup = fp32_latency / fp16_latency
                print(f"   TensorRT-FP16: {fp16_latency:.2f}ms, {fp16_throughput:.0f} images/sec ({fp16_speedup:.2f}x)")
            else:
                fp16_latency = fp32_latency
                fp16_throughput = fp32_throughput
                fp16_speedup = 1.0
            
            # 3. PTQ4ViTé‡åŒ–æ¨¡å‹
            print("3ï¸âƒ£ æµ‹è¯•PTQ4ViTé‡åŒ–æ¨¡å‹")
            quantized_model = apply_ptq4vit_quantization(model_name, calib_size=32)
            ptq_latency = test_model_latency(quantized_model, bs, "PTQ4ViT")
            ptq_throughput = bs * 1000 / ptq_latency
            ptq_vs_fp32 = fp32_latency / ptq_latency
            print(f"   PTQ4ViT: {ptq_latency:.2f}ms, {ptq_throughput:.0f} images/sec ({ptq_vs_fp32:.2f}x vs FP32)")
            
            # 4. PTQ4ViT + TensorRT INT8ä¼˜åŒ–
            print("4ï¸âƒ£ æµ‹è¯•PTQ4ViT + TensorRT INT8ä¼˜åŒ–")
            trt_int8_model, success = compile_tensorrt_int8(quantized_model, input_shape, use_int8=True)
            if success:
                int8_latency = test_model_latency(trt_int8_model, bs, "TensorRT-INT8")
                int8_throughput = bs * 1000 / int8_latency
                int8_speedup = fp32_latency / int8_latency
                int8_vs_ptq = ptq_latency / int8_latency
                print(f"   TensorRT-INT8: {int8_latency:.2f}ms, {int8_throughput:.0f} images/sec ({int8_speedup:.2f}x vs FP32, {int8_vs_ptq:.2f}x vs PTQ)")
            else:
                int8_latency = ptq_latency
                int8_throughput = ptq_throughput
                int8_speedup = ptq_vs_fp32
                int8_vs_ptq = 1.0
            
            results.append({
                'batch_size': bs,
                'fp32_latency': fp32_latency,
                'fp32_throughput': fp32_throughput,
                'fp16_latency': fp16_latency,
                'fp16_throughput': fp16_throughput,
                'fp16_speedup': fp16_speedup,
                'ptq_latency': ptq_latency,
                'ptq_throughput': ptq_throughput,
                'ptq_speedup': ptq_vs_fp32,
                'int8_latency': int8_latency,
                'int8_throughput': int8_throughput,
                'int8_speedup': int8_speedup,
                'int8_vs_ptq': int8_vs_ptq,
            })
            
            # æ¸…ç†æ˜¾å­˜
            del original_model, trt_fp16_model, quantized_model, trt_int8_model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
            torch.cuda.empty_cache()
            continue
    
    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“ˆ é‡åŒ–åŠ é€Ÿæ•ˆæœæ±‡æ€»:")
    print(f"{'BS':<4} {'FP32(ms)':<10} {'FP16(ms)':<10} {'PTQ(ms)':<10} {'INT8(ms)':<10} {'FP16â†‘':<8} {'PTQâ†‘':<8} {'INT8â†‘':<8} {'é¢å¤–â†‘':<8}")
    print("-" * 90)
    
    for r in results:
        print(f"{r['batch_size']:<4} "
              f"{r['fp32_latency']:<10.2f} "
              f"{r['fp16_latency']:<10.2f} "
              f"{r['ptq_latency']:<10.2f} "
              f"{r['int8_latency']:<10.2f} "
              f"{r['fp16_speedup']:<8.2f}x "
              f"{r['ptq_speedup']:<8.2f}x "
              f"{r['int8_speedup']:<8.2f}x "
              f"{r['int8_vs_ptq']:<8.2f}x")
    
    return results

def main():
    print("ğŸ”§ PTQ4ViT + TensorRT INT8 åŠ é€Ÿæµ‹è¯•")
    print("="*80)
    
    try:
        results = compare_quantization_acceleration()
        
        print(f"\nğŸ’¡ ç»“è®º:")
        print(f"âœ… æˆåŠŸæµ‹è¯•äº†å®Œæ•´çš„é‡åŒ–+TensorRTæµç¨‹")
        print(f"ğŸ“Š å¯ä»¥çœ‹åˆ°PTQ4ViTé‡åŒ–çš„ç²¾åº¦æŸå¤±å’ŒTensorRTçš„åŠ é€Ÿæ•ˆæœ")
        print(f"ğŸš€ INT8é‡åŒ–+TensorRTæä¾›äº†æœ€ä½³çš„æ€§èƒ½è¡¨ç°")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()