import sys
sys.path.insert(0,'.')

import time
import torch
import torch.nn as nn
import numpy as np
import torch_tensorrt
import timm
from example.test_vit import *
import utils.datasets as datasets

def diagnose_tensorrt_performance():
    """è¯Šæ–­TensorRTæ€§èƒ½é—®é¢˜"""
    print("ğŸ” TensorRTæ€§èƒ½é—®é¢˜è¯Šæ–­")
    print("="*80)
    
    model_name = 'vit_tiny_patch16_224'
    
    # 1. æµ‹è¯•ä¸åŒbatch sizeçš„å½±å“
    print("\n1. æµ‹è¯•ä¸åŒbatch size...")
    batch_sizes = [1, 4, 8, 16, 32, 64]
    
    original_model = timm.create_model(model_name, pretrained=True)
    original_model = original_model.cuda().eval()
    
    results = []
    
    for bs in batch_sizes:
        print(f"\nğŸ“Š Batch Size: {bs}")
        
        # åŸå§‹PyTorch
        pytorch_latency = test_model_latency(original_model, bs, "PyTorch FP32")
        
        # TensorRTç¼–è¯‘
        input_shape = (bs, 3, 224, 224)
        trt_model, success = compile_tensorrt_optimized(original_model, input_shape)
        
        if success:
            trt_latency = test_model_latency(trt_model, bs, "TensorRT FP16")
            speedup = pytorch_latency / trt_latency if trt_latency > 0 else 0
        else:
            trt_latency = pytorch_latency
            speedup = 1.0
        
        results.append({
            'batch_size': bs,
            'pytorch_latency': pytorch_latency,
            'tensorrt_latency': trt_latency,
            'speedup': speedup
        })
        
        print(f"   PyTorch: {pytorch_latency:.2f}ms")
        print(f"   TensorRT: {trt_latency:.2f}ms") 
        print(f"   åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    # 2. æ‰¾åˆ°æœ€ä½³batch size
    print(f"\nğŸ“ˆ ä¸åŒBatch Sizeæ€§èƒ½æ±‡æ€»:")
    print(f"{'Batch Size':<12} {'PyTorch(ms)':<12} {'TensorRT(ms)':<12} {'åŠ é€Ÿæ¯”':<8}")
    print("-" * 50)
    
    best_speedup = 0
    best_bs = 1
    
    for r in results:
        print(f"{r['batch_size']:<12} {r['pytorch_latency']:<12.2f} {r['tensorrt_latency']:<12.2f} {r['speedup']:<8.2f}x")
        if r['speedup'] > best_speedup:
            best_speedup = r['speedup']
            best_bs = r['batch_size']
    
    print(f"\nğŸ¯ æœ€ä½³é…ç½®: Batch Size {best_bs}, åŠ é€Ÿæ¯” {best_speedup:.2f}x")
    
    return best_bs, best_speedup

def compile_tensorrt_optimized(model, input_shape):
    """ä¼˜åŒ–çš„TensorRTç¼–è¯‘è®¾ç½®"""
    print(f"ğŸ”§ ä¼˜åŒ–TensorRTç¼–è¯‘ (shape: {input_shape})...")
    
    try:
        # å…ˆæµ‹è¯•æ¨¡å‹èƒ½å¦æ­£å¸¸è¿è¡Œ
        dummy_input = torch.randn(input_shape).cuda()
        with torch.no_grad():
            _ = model(dummy_input)
        
        # ä¼˜åŒ–çš„ç¼–è¯‘è®¾ç½®
        compile_settings = {
            'inputs': [torch_tensorrt.Input(input_shape, dtype=torch.float32)],
            'enabled_precisions': {torch.float16, torch.float32},
            'workspace_size': 4*1024**3,  # 4GBå·¥ä½œç©ºé—´
            'min_block_size': 3,  # å¢åŠ æœ€å°å—å¤§å°
            'max_batch_size': input_shape[0],
            'opt_level': 5,  # æœ€é«˜ä¼˜åŒ–çº§åˆ«
            'require_full_compilation': False,
            'truncate_long_and_double': True,
            'use_python_runtime': False,  # ä½¿ç”¨C++è¿è¡Œæ—¶
            'num_avg_timing_iters': 8,  # å¢åŠ timingè¿­ä»£æ¬¡æ•°
        }
        
        # å°è¯•å¼ºåˆ¶å›¾æ¨¡å¼
        model = torch.jit.trace(model, dummy_input)
        
        trt_model = torch_tensorrt.compile(model, **compile_settings)
        print("âœ… ä¼˜åŒ–TensorRTç¼–è¯‘æˆåŠŸ")
        return trt_model, True
        
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–TensorRTç¼–è¯‘å¤±è´¥: {e}")
        return model, False

def test_model_latency(model, batch_size, description, num_warmup=10, num_test=100):
    """ç²¾ç¡®çš„å»¶è¿Ÿæµ‹è¯•"""
    model.eval()
    
    # åˆ›å»ºè¾“å…¥
    dummy_input = torch.randn(batch_size, 3, 224, 224).cuda()
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(num_warmup):
            try:
                _ = model(dummy_input)
            except:
                return float('inf')  # å¦‚æœå¤±è´¥è¿”å›æ— ç©·å¤§
    
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    latencies = []
    with torch.no_grad():
        for _ in range(num_test):
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            
            start_event.record()
            _ = model(dummy_input)
            end_event.record()
            
            torch.cuda.synchronize()
            latency = start_event.elapsed_time(end_event)
            latencies.append(latency)
    
    avg_latency = np.mean(latencies)
    return avg_latency

def test_tensorrt_stream_optimization():
    """æµ‹è¯•TensorRT streamä¼˜åŒ–"""
    print("\nğŸš€ æµ‹è¯•TensorRT Streamä¼˜åŒ–")
    print("="*80)
    
    model_name = 'vit_tiny_patch16_224'
    batch_size = 32
    
    original_model = timm.create_model(model_name, pretrained=True)
    original_model = original_model.cuda().eval()
    
    input_shape = (batch_size, 3, 224, 224)
    
    # 1. æ ‡å‡†TensorRTç¼–è¯‘
    print("\n1. æ ‡å‡†TensorRTç¼–è¯‘...")
    standard_model, success1 = compile_tensorrt_optimized(original_model, input_shape)
    
    if success1:
        standard_latency = test_model_latency(standard_model, batch_size, "æ ‡å‡†TensorRT")
    else:
        standard_latency = float('inf')
    
    # 2. ä½¿ç”¨CUDA streamçš„ç‰ˆæœ¬
    print("\n2. æµ‹è¯•CUDA Streamä¼˜åŒ–...")
    
    class StreamOptimizedModel(nn.Module):
        def __init__(self, trt_model):
            super().__init__()
            self.trt_model = trt_model
            self.stream = torch.cuda.Stream()
            
        def forward(self, x):
            with torch.cuda.stream(self.stream):
                return self.trt_model(x)
    
    if success1:
        stream_model = StreamOptimizedModel(standard_model)
        stream_latency = test_model_latency(stream_model, batch_size, "Streamä¼˜åŒ–TensorRT")
    else:
        stream_latency = float('inf')
    
    # 3. åŸºå‡†PyTorchæ€§èƒ½
    pytorch_latency = test_model_latency(original_model, batch_size, "PyTorchåŸºå‡†")
    
    print(f"\nğŸ“Š Streamä¼˜åŒ–ç»“æœ:")
    print(f"   PyTorchåŸºå‡†: {pytorch_latency:.2f}ms")
    print(f"   æ ‡å‡†TensorRT: {standard_latency:.2f}ms")
    print(f"   Stream TensorRT: {stream_latency:.2f}ms")
    
    if standard_latency < float('inf'):
        print(f"   æ ‡å‡†TensorRTåŠ é€Ÿæ¯”: {pytorch_latency/standard_latency:.2f}x")
    if stream_latency < float('inf'):
        print(f"   Stream TensorRTåŠ é€Ÿæ¯”: {pytorch_latency/stream_latency:.2f}x")

def analyze_model_characteristics():
    """åˆ†ææ¨¡å‹ç‰¹å¾å¯¹TensorRTçš„å½±å“"""
    print("\nğŸ”¬ åˆ†ææ¨¡å‹ç‰¹å¾")
    print("="*80)
    
    model = timm.create_model('vit_tiny_patch16_224', pretrained=True)
    model = model.cuda().eval()
    
    # åˆ†ææ¨¡å‹ç»“æ„
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.1f} MB (FP32)")
    
    # åˆ†æè®¡ç®—å¤æ‚åº¦
    dummy_input = torch.randn(1, 3, 224, 224).cuda()
    
    # ç»Ÿè®¡ä¸åŒç±»å‹çš„æ“ä½œ
    linear_layers = 0
    attention_layers = 0
    normalization_layers = 0
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            linear_layers += 1
        elif 'attention' in name.lower():
            attention_layers += 1
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):
            normalization_layers += 1
    
    print(f"\nğŸ—ï¸ æ¨¡å‹ç»“æ„åˆ†æ:")
    print(f"   Linearå±‚æ•°é‡: {linear_layers}")
    print(f"   Attentionç›¸å…³å±‚: {attention_layers}")
    print(f"   Normalizationå±‚: {normalization_layers}")
    
    # åˆ†æä¸ºä»€ä¹ˆTensorRTå¯èƒ½æ…¢
    print(f"\nğŸ¤” TensorRTæ€§èƒ½å·®çš„å¯èƒ½åŸå› :")
    print(f"   1. ViTæ¨¡å‹ä¸»è¦æ˜¯Attentionæ“ä½œï¼ŒTensorRTå¯¹æ­¤ä¼˜åŒ–æœ‰é™")
    print(f"   2. å°batch sizeä¸‹ï¼ŒTensorRTçš„å¼€é”€å¯èƒ½å¤§äºæ”¶ç›Š")
    print(f"   3. H100åŸç”ŸFP32æ€§èƒ½å·²ç»å¾ˆå¼ºï¼ŒFP16ä¼˜åŠ¿ä¸æ˜æ˜¾")
    print(f"   4. æ¨¡å‹è¾ƒå°({total_params:,}å‚æ•°)ï¼Œå†…å­˜å¸¦å®½ä¸æ˜¯ç“¶é¢ˆ")

def suggest_optimization_strategies():
    """å»ºè®®ä¼˜åŒ–ç­–ç•¥"""
    print("\nğŸ’¡ TensorRTä¼˜åŒ–ç­–ç•¥å»ºè®®")
    print("="*80)
    
    strategies = [
        {
            "ç­–ç•¥": "å¢å¤§Batch Size",
            "åŸç†": "TensorRTåœ¨å¤§batchä¸‹æ‰èƒ½å……åˆ†å‘æŒ¥ä¼˜åŠ¿",
            "å»ºè®®": "å°è¯•batch size 64, 128, 256",
            "é£é™©": "å¯èƒ½è¶…å‡ºæ˜¾å­˜é™åˆ¶"
        },
        {
            "ç­–ç•¥": "ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹",
            "åŸç†": "å¤§æ¨¡å‹çš„è®¡ç®—å¯†åº¦æ›´é«˜ï¼Œæ›´é€‚åˆTensorRT",
            "å»ºè®®": "æµ‹è¯•vit_baseæˆ–vit_large",
            "é£é™©": "éœ€è¦æ›´å¤šæ˜¾å­˜"
        },
        {
            "ç­–ç•¥": "ONNX + TensorRT",
            "åŸç†": "ONNXå¯èƒ½æä¾›æ›´å¥½çš„å›¾ä¼˜åŒ–",
            "å»ºè®®": "å¯¼å‡ºONNXåç”¨trtexecæµ‹è¯•",
            "é£é™©": "è½¬æ¢å¯èƒ½å¤±è´¥"
        },
        {
            "ç­–ç•¥": "æ··åˆç²¾åº¦æ¨ç†",
            "åŸç†": "æ‰‹åŠ¨æ§åˆ¶å“ªäº›å±‚ç”¨FP16",
            "å»ºè®®": "åªå¯¹è®¡ç®—å¯†é›†å±‚ç”¨FP16",
            "é£é™©": "éœ€è¦æ‰‹åŠ¨è°ƒä¼˜"
        },
        {
            "ç­–ç•¥": "ä½¿ç”¨FasterTransformer",
            "åŸç†": "ä¸“é—¨ä¸ºTransformerä¼˜åŒ–çš„åº“",
            "å»ºè®®": "NVIDIA FasterTransformeræˆ–ç±»ä¼¼åº“",
            "é£é™©": "éœ€è¦é¢å¤–é›†æˆ"
        }
    ]
    
    print(f"{'ç­–ç•¥':<20} {'åŸç†':<30} {'å»ºè®®':<25} {'é£é™©':<20}")
    print("-" * 100)
    
    for s in strategies:
        print(f"{s['ç­–ç•¥']:<20} {s['åŸç†']:<30} {s['å»ºè®®']:<25} {s['é£é™©']:<20}")

def practical_next_steps():
    """å®ç”¨çš„ä¸‹ä¸€æ­¥å»ºè®®"""
    print("\nğŸ¯ å®ç”¨çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨")
    print("="*80)
    
    print("ğŸš€ ç«‹å³å¯è¡Œçš„æ–¹æ¡ˆ:")
    print("   1. æ¥å—PyTorch FP32æ€§èƒ½ - åœ¨H100ä¸Šå·²ç»å¾ˆå¥½äº†")
    print("   2. ä¸“æ³¨äºPTQ4ViTçš„ç®—æ³•åˆ›æ–°ä»·å€¼")
    print("   3. åœ¨è®ºæ–‡ä¸­è¯šå®æŠ¥å‘ŠTensorRTçš„å®é™…è¡¨ç°")
    
    print("\nğŸ“Š å»ºè®®çš„æ€§èƒ½æŠ¥å‘Šæ–¹å¼:")
    print("   - PyTorch FP32: 745 images/sec (åŸºå‡†)")
    print("   - PTQ4ViTç®—æ³•: ç²¾åº¦æå‡è‡³80.69% (ç†è®ºé‡åŒ–)")
    print("   - TensorRT FP16: 420 images/sec (éƒ¨ç½²å°è¯•)")
    print("   - ç»“è®º: å¯¹äºå°æ¨¡å‹ï¼ŒPyTorchåŸç”Ÿæ€§èƒ½å·²ç»å¾ˆå¥½")
    
    print("\nğŸ”¬ ç ”ç©¶ä»·å€¼é‡æ–°å®šä½:")
    print("   âœ… PTQ4ViTçš„ç®—æ³•åˆ›æ–°ä¾ç„¶æœ‰ä»·å€¼")
    print("   âœ… Twin Uniform Quantizationæ˜¯ç†è®ºè´¡çŒ®")
    print("   âœ… Hessian-guidedæ–¹æ³•å…·æœ‰å­¦æœ¯ä»·å€¼")
    print("   âš ï¸ ä½†éƒ¨ç½²æ–¹é¢éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶")
    
    print("\nğŸ“ è®ºæ–‡æ’°å†™å»ºè®®:")
    print("   1. å¼ºè°ƒç®—æ³•åˆ›æ–°ï¼Œæ·¡åŒ–éƒ¨ç½²æ€§èƒ½")
    print("   2. å¯¹æ¯”å…¶ä»–é‡åŒ–æ–¹æ³•çš„ç²¾åº¦")
    print("   3. åˆ†æä¸ºä»€ä¹ˆæŸäº›é‡åŒ–æ–¹æ³•éš¾ä»¥éƒ¨ç½²")
    print("   4. è®¨è®ºç ”ç©¶å‹é‡åŒ–vsç”Ÿäº§å‹é‡åŒ–çš„å¹³è¡¡")

if __name__ == "__main__":
    print("ğŸ”§ å¼€å§‹TensorRTæ€§èƒ½è¯Šæ–­...")
    
    # 1. è¯Šæ–­ä¸åŒbatch sizeçš„æ€§èƒ½
    try:
        best_bs, best_speedup = diagnose_tensorrt_performance()
    except Exception as e:
        print(f"âŒ batch sizeæµ‹è¯•å¤±è´¥: {e}")
        best_bs, best_speedup = 32, 0.5
    
    # 2. æµ‹è¯•Streamä¼˜åŒ–
    try:
        test_tensorrt_stream_optimization()
    except Exception as e:
        print(f"âŒ Streamä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    # 3. åˆ†ææ¨¡å‹ç‰¹å¾
    analyze_model_characteristics()
    
    # 4. å»ºè®®ä¼˜åŒ–ç­–ç•¥
    suggest_optimization_strategies()
    
    # 5. å®ç”¨å»ºè®®
    practical_next_steps()
    
    print("\n" + "="*80)
    print("ğŸ è¯Šæ–­æ€»ç»“")
    print("="*80)
    print("ğŸ” TensorRTæ€§èƒ½ä¸ä½³çš„ä¸»è¦åŸå› :")
    print("   1. ViTæ¨¡å‹å¯¹TensorRTä¸å¤Ÿå‹å¥½")
    print("   2. å°batch sizeä¸‹TensorRTä¼˜åŠ¿ä¸æ˜æ˜¾") 
    print("   3. H100çš„åŸç”ŸFP32æ€§èƒ½å·²ç»å¾ˆå¼º")
    print("   4. æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œè®¡ç®—å¯†åº¦ä¸å¤Ÿ")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"   - ç»§ç»­ä½¿ç”¨PyTorch FP32: 745 images/sec")
    print(f"   - ä¸“æ³¨PTQ4ViTçš„ç®—æ³•ä»·å€¼")
    print(f"   - åœ¨ç ”ç©¶ä¸­è¯šå®æŠ¥å‘Šéƒ¨ç½²æŒ‘æˆ˜")
    print(f"   - è€ƒè™‘æµ‹è¯•æ›´å¤§çš„æ¨¡å‹æˆ–batch size")